{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TableTennis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size,fc1,fc2, seed):\n",
        "\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        self.fc1=nn.Linear(state_size,fc1)\n",
        "        self.fc2=nn.Linear(fc1+action_size,fc2)\n",
        "        \n",
        "        self.bn=nn.BatchNorm1d(state_size)\n",
        "        self.bn2=nn.BatchNorm1d(fc1)\n",
        " \n",
        "        self.fc5=nn.Linear(fc2,1)\n",
        "        \n",
        "        #last layer weight and bias initialization \n",
        "        self.fc5.weight.data.uniform_(-3e-4, 3e-4)\n",
        "        self.fc5.bias.data.uniform_(-3e-4, 3e-4)\n",
        "        \n",
        "        #torch.nn.init.uniform_(self.fc5.weight, a=-3e-4, b=3e-4)\n",
        "        #torch.nn.init.uniform_(self.fc5.bias, a=-3e-4, b=3e-4)\n",
        "        \n",
        "    def forward(self, state,action):\n",
        "        \n",
        "        x=self.bn(state)\n",
        "        x=F.relu(self.bn2(self.fc1(x)))\n",
        "        x=torch.cat([x,action],1)\n",
        "        x=F.relu(self.fc2(x))\n",
        "        \n",
        "        x=self.fc5(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    \n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self,state_size, action_size, fc1,fc2,seed):\n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "\n",
        "        # network mapping state to action \n",
        "\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        \n",
        "        self.bn=nn.BatchNorm1d(state_size)\n",
        "        self.bn2=nn.BatchNorm1d(fc1)\n",
        "        self.bn3=nn.BatchNorm1d(fc2)\n",
        "        \n",
        "        self.fc1= nn.Linear(state_size,fc1)\n",
        "        self.fc2 = nn.Linear(fc1,fc2)\n",
        "        self.fc4 = nn.Linear(fc2, action_size)\n",
        "        \n",
        "        #last layer weight and bias initialization \n",
        "        torch.nn.init.uniform_(self.fc4.weight, a=-3e-3, b=3e-3)\n",
        "        torch.nn.init.uniform_(self.fc4.bias, a=-3e-3, b=3e-3)\n",
        "        \n",
        "        # Tanh\n",
        "        self.tan = nn.Tanh()\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x=self.bn(x)\n",
        "        x = F.relu(self.bn2(self.fc1(x)))\n",
        "        x = F.relu(self.bn3(self.fc2(x)))\n",
        "\n",
        "        return self.tan(self.fc4(x))\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "Oep2oqlEIJru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn-ZaLI_HkSJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "from model import Critic, Actor\n",
        "import torch\n",
        "from torch import autograd\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random_p as rm\n",
        "from schedule import LinearSchedule\n",
        "\n",
        "BUFFER_SIZE = int(1e6)  \n",
        "BATCH_SIZE = 512         \n",
        "GAMMA = 0.99            \n",
        "TAU = 1e-3              \n",
        "ACTOR_LR = 1e-3        \n",
        "CRITIC_LR = 1e-4        \n",
        "UPDATE_EVERY = 20      \n",
        "#UPDATE_TIMES = 10       \n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Agent():\n",
        "\n",
        "\n",
        "    def __init__(self, state_size, action_size, num_agents,seed,fc1=400,fc2=300,update_times=10,weight_decay=1.e-5):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.num_agents=num_agents\n",
        "        self.n_seed=np.random.seed(seed)\n",
        "        self.update_times=update_times\n",
        "        self.n_step=0\n",
        "        self.agents = []\n",
        "        \n",
        "        self.noise=[]\n",
        "        for i in range(num_agents):\n",
        "            self.noise.append(rm.OrnsteinUhlenbeckProcess(size=(action_size, ), std=LinearSchedule(0.4)))\n",
        "\n",
        "\n",
        "        self.critic_local = Critic(state_size, action_size,fc1,fc2, seed).to(device)\n",
        "        \n",
        "        self.critic_target = Critic(state_size, action_size,fc1,fc2, seed).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic_local.state_dict())\n",
        "        \n",
        "        self.actor_local=Actor(state_size, action_size,fc1,fc2, seed).to(device)\n",
        "        self.actor_target=Actor(state_size, action_size,fc1,fc2, seed).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor_local.state_dict())\n",
        "        \n",
        "\n",
        "        self.optimizer_critic = optim.Adam(self.critic_local.parameters(), lr=CRITIC_LR,weight_decay=1.e-5)\n",
        "        self.optimizer_actor = optim.Adam(self.actor_local.parameters(), lr=ACTOR_LR)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        \n",
        "\n",
        "        self.t_step = 0\n",
        "        self.a_step = 0\n",
        "\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        for i in range(self.num_agents):\n",
        "            all_state=self.memory.add(state[i], action[i], reward[i], next_state[i], done[i])\n",
        "        \n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        \n",
        "        if self.t_step == 0:\n",
        "            \n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE: \n",
        "                for i in range(self.update_times):\n",
        "                    experiences = self.memory.sample()\n",
        "                    self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state,training=True):\n",
        "\n",
        "        state = torch.from_numpy(state).float().detach().to(device)\n",
        "        #print(state.shape,\"act\")\n",
        "        \n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            actions=self.actor_local(state)\n",
        "        self.actor_local.train()\n",
        "        \n",
        "        self.n_step+=1\n",
        "        dec=1.08*max((500-self.n_step)/500,.05)\n",
        "        noise=[]\n",
        "        #for i in range7(self.num_agents):\n",
        "        #    noise.append(self.noise[i].sample())\n",
        "        \n",
        "        return np.clip(actions.cpu().data.numpy()+np.random.uniform(-1,1,(self.num_agents,self.action_size))*dec,-1,1)\n",
        "        #np.clip(actions.cpu().data.numpy()+np.array(noise),-1,1)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "\n",
        "        \n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        next_actions=self.actor_target(next_states)\n",
        "        with torch.no_grad():\n",
        "            Q_target_next = self.critic_target(next_states,next_actions)\n",
        "        Q_targets= rewards +(gamma * Q_target_next * (1-dones))\n",
        "        \n",
        "        Q_expected = self.critic_local(states,actions)\n",
        "        \n",
        "        #critic loss\n",
        "        loss=F.mse_loss(Q_expected, Q_targets.detach())\n",
        "        \n",
        "        self.optimizer_critic.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
        "        self.optimizer_critic.step()\n",
        "        \n",
        "        #actor loss\n",
        "        \n",
        "        \n",
        "        \n",
        "        action_pr = self.actor_local(states)\n",
        "        p_loss=-self.critic_local(states,action_pr).mean()\n",
        "\n",
        "        \n",
        "        \n",
        "        self.optimizer_actor.zero_grad()\n",
        "        p_loss.backward()\n",
        "        \n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "\n",
        "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
        "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
        "\n",
        "    def reset(self):\n",
        "        self.actor_target.load_state_dict(self.actor_local.state_dict())\n",
        "        self.critic_target.load_state_dict(self.critic_local.state_dict())\n",
        "        self.t_step = 1\n",
        "        self.a_step = 1\n",
        "        \n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "            \n",
        "    def reset_random(self):\n",
        "        for i in range(self.num_agents):\n",
        "            self.noise[i].reset_states()\n",
        "        \n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        #actions.requires_grad=True\n",
        "        #print(actions.requires_grad,\"grad\")\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "  \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unityagents import UnityEnvironment\n",
        "import numpy as np\n",
        "env = UnityEnvironment(file_name=\"C:/Users/itsra/google-football-pytorch/Tennis-MultiAgent/Tennis_Windows_x86_64/Tennis.exe\",no_graphics=True) \n",
        "brain_name = env.brain_names[0]\n",
        "brain = env.brains[brain_name]\n",
        "\n",
        "env_info = env.reset(train_mode=True)[brain_name]\n",
        "\n",
        "\n",
        "num_agents = len(env_info.agents)\n",
        "print('Number of agents:', num_agents)\n",
        "\n",
        "\n",
        "action_size = brain.vector_action_space_size\n",
        "print('Size of each action:', action_size)\n",
        "\n",
        "\n",
        "states = env_info.vector_observations\n",
        "state_size = states.shape[1]\n",
        "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
        "print('The state for the first agent looks like:', states[0])\n",
        "\n",
        "\n",
        "for i in range(1, 6):                                      \n",
        "    env_info = env.reset(train_mode=False)[brain_name]     \n",
        "    states = env_info.vector_observations                  \n",
        "    scores = np.zeros(num_agents)                          \n",
        "    while True:\n",
        "        actions = np.random.randn(num_agents, action_size) \n",
        "        actions = np.clip(actions, -1, 1)                 \n",
        "        env_info = env.step(actions)[brain_name]           \n",
        "        next_states = env_info.vector_observations         \n",
        "        rewards = env_info.rewards                        \n",
        "        dones = env_info.local_done                        \n",
        "        scores += env_info.rewards                         \n",
        "        states = next_states                               \n",
        "        if np.any(dones):                                 \n",
        "            break\n",
        "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))\n",
        "\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ddpg_agent import Agent\n",
        "agent = Agent(state_size, action_size,num_agents,fc1=400,fc2=300, seed=0)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"working on gpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"working on cpu\")\n",
        "def ddpg(n_episodes=5000):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "    \"\"\"\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
        "        agent.reset_random()              #reset noise object\n",
        "        state = env_info.vector_observations\n",
        "        score = 0\n",
        "        t=0\n",
        "        print(i_episode)\n",
        "        while True:\n",
        "            t=t+1\n",
        "            action=agent.act(state)\n",
        "            #print(action)\n",
        "            env_info = env.step(np.array(action))[brain_name] \n",
        "            next_state = env_info.vector_observations   # get the next state\n",
        "            reward = env_info.rewards                   # get the reward\n",
        "\n",
        "            done = env_info.local_done\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            #print(reward)\n",
        "            score += max(reward)\n",
        "            if np.any(done):\n",
        "                break \n",
        "\n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 300 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=1:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "\n",
        "            torch.save({\n",
        "                        'model_state_dict': agent.critic_local.state_dict(),\n",
        "                        'optimizer_state_dict': agent.optimizer_critic.state_dict(),\n",
        "                        }, 'trained_weights/checkpoint_critic.pth')\n",
        "\n",
        "            torch.save({\n",
        "                        'model_state_dict': agent.actor_local.state_dict(),\n",
        "                        'optimizer_state_dict': agent.optimizer_actor.state_dict(),\n",
        "                        }, 'trained_weights/checkpoint_actor.pth')\n",
        "            break\n",
        "    return scores\n",
        "scores = ddpg()\n",
        "\n",
        "# plot the scores\n",
        "#fig = plt.figure()\n",
        "#ax = fig.add_subplot(111)\n",
        "#plt.plot(np.arange(len(scores)), scores)\n",
        "#plt.ylabel('Score')\n",
        "#plt.xlabel('Episode #')\n",
        "#plt.show()\n",
        "\n",
        "'''\n",
        "agent.critic_local.load_state_dict(torch.load('trained_weights/checkpoint_critic.pth'))\n",
        "agent.actor_local.load_state_dict(torch.load('trained_weights/checkpoint_actor.pth'))\n",
        "\n",
        "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
        "state = env_info.vector_observations            # get the current state\n",
        "#print(state.shape)\n",
        "score = 0    \n",
        "reward_i=[]# initialize the score\n",
        "past_a=deque(maxlen=5)\n",
        "while True:\n",
        "    action=[]\n",
        "\n",
        "    action=agent.act(state)        # select an action\n",
        "    env_info = env.step(np.array(action))[brain_name]        # send the action to the environment\n",
        "    next_state = env_info.vector_observations   # get the next state\n",
        "    reward = env_info.rewards                  # get the reward\n",
        "    done = env_info.local_done                 # see if episode has finished\n",
        "    score += np.mean(reward)                   # update the score\n",
        "    state = next_state                         # roll over the state to next time step\n",
        "\n",
        "    if np.any(done):                           # exit loop if episode finished\n",
        "        break\n",
        "    \n",
        "print(\"Score: {}\".format(score))'''\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "mZ3ID52EIU2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}