{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleFootball.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKpgI91XIkqM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class deepmind(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(deepmind, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(16, 32, 8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 32, 3, stride=1)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 8, 512)        \n",
        "        # start to do the init...\n",
        "        nn.init.orthogonal_(self.conv1.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        nn.init.orthogonal_(self.conv2.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        nn.init.orthogonal_(self.conv3.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        nn.init.orthogonal_(self.fc1.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "        # init the bias...\n",
        "        nn.init.constant_(self.conv1.bias.data, 0)\n",
        "        nn.init.constant_(self.conv2.bias.data, 0)\n",
        "        nn.init.constant_(self.conv3.bias.data, 0)\n",
        "        nn.init.constant_(self.fc1.bias.data, 0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 32 * 5 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class cnn_net(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super(cnn_net, self).__init__()\n",
        "        self.cnn_layer = deepmind()\n",
        "        self.critic = nn.Linear(512, 1)\n",
        "        self.actor = nn.Linear(512, num_actions)\n",
        "\n",
        "        # init the linear layer..\n",
        "        nn.init.orthogonal_(self.critic.weight.data)\n",
        "        nn.init.constant_(self.critic.bias.data, 0)\n",
        "        # init the policy layer...\n",
        "        nn.init.orthogonal_(self.actor.weight.data, gain=0.01)\n",
        "        nn.init.constant_(self.actor.bias.data, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.cnn_layer(inputs / 255.0)\n",
        "        value = self.critic(x)\n",
        "        pi = F.softmax(self.actor(x), dim=1)\n",
        "        return value, pi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from utils import select_actions, evaluate_actions, config_logger\n",
        "from datetime import datetime\n",
        "import os\n",
        "import copy\n",
        "\n",
        "class ppo_agent:\n",
        "    def __init__(self, envs, args, net):\n",
        "        self.envs = envs \n",
        "        self.args = args\n",
        "        self.net = net\n",
        "        self.old_net = copy.deepcopy(self.net)\n",
        "        if self.args.cuda:\n",
        "            self.net.cuda()\n",
        "            self.old_net.cuda()\n",
        "        self.optimizer = optim.Adam(self.net.parameters(), self.args.lr, eps=self.args.eps)\n",
        "        if not os.path.exists(self.args.save_dir):\n",
        "            os.mkdir(self.args.save_dir)\n",
        "        self.model_path = os.path.join(self.args.save_dir, self.args.env_name)\n",
        "        if not os.path.exists(self.model_path):\n",
        "            os.mkdir(self.model_path)\n",
        "        if not os.path.exists(self.args.log_dir):\n",
        "            os.mkdir(self.args.log_dir)\n",
        "        self.log_path = self.args.log_dir + self.args.env_name + '.log'\n",
        "        self.batch_ob_shape = (self.args.num_workers * self.args.nsteps, ) + self.envs.observation_space.shape\n",
        "        self.obs = np.zeros((self.args.num_workers, ) + self.envs.observation_space.shape, dtype=self.envs.observation_space.dtype.name)\n",
        "        self.obs[:] = self.envs.reset()\n",
        "        self.dones = [False for _ in range(self.args.num_workers)]\n",
        "        self.logger = config_logger(self.log_path)\n",
        "\n",
        "    def learn(self):\n",
        "        num_updates = self.args.total_frames // (self.args.nsteps * self.args.num_workers)\n",
        "        episode_rewards = torch.zeros([self.args.num_workers, 1])\n",
        "        final_rewards = torch.zeros([self.args.num_workers, 1])\n",
        "        for update in range(num_updates):\n",
        "            mb_obs, mb_rewards, mb_actions, mb_dones, mb_values = [], [], [], [], []\n",
        "            if self.args.lr_decay:\n",
        "                self._adjust_learning_rate(update, num_updates)\n",
        "            for step in range(self.args.nsteps):\n",
        "                with torch.no_grad():\n",
        "                    obs_tensor = self._get_tensors(self.obs)\n",
        "                    values, pis = self.net(obs_tensor)\n",
        "                actions = select_actions(pis)\n",
        "                input_actions = actions \n",
        "                mb_obs.append(np.copy(self.obs))\n",
        "                mb_actions.append(actions)\n",
        "                mb_dones.append(self.dones)\n",
        "                mb_values.append(values.detach().cpu().numpy().squeeze())\n",
        "                obs, rewards, dones, _ = self.envs.step(input_actions)\n",
        "                self.dones = dones\n",
        "                mb_rewards.append(rewards)\n",
        "                for n, done in enumerate(dones):\n",
        "                    if done:\n",
        "                        self.obs[n] = self.obs[n] * 0\n",
        "                self.obs = obs\n",
        "                rewards = torch.tensor(np.expand_dims(np.stack(rewards), 1), dtype=torch.float32)\n",
        "                episode_rewards += rewards\n",
        "                masks = torch.tensor([[0.0] if done_ else [1.0] for done_ in dones], dtype=torch.float32)\n",
        "                final_rewards *= masks\n",
        "                final_rewards += (1 - masks) * episode_rewards\n",
        "                episode_rewards *= masks\n",
        "            mb_obs = np.asarray(mb_obs, dtype=np.float32)\n",
        "            mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
        "            mb_actions = np.asarray(mb_actions, dtype=np.float32)\n",
        "            mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
        "            mb_values = np.asarray(mb_values, dtype=np.float32)\n",
        "            with torch.no_grad():\n",
        "                obs_tensor = self._get_tensors(self.obs)\n",
        "                last_values, _ = self.net(obs_tensor)\n",
        "                last_values = last_values.detach().cpu().numpy().squeeze()\n",
        "            mb_returns = np.zeros_like(mb_rewards)\n",
        "            mb_advs = np.zeros_like(mb_rewards)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(self.args.nsteps)):\n",
        "                if t == self.args.nsteps - 1:\n",
        "                    nextnonterminal = 1.0 - self.dones\n",
        "                    nextvalues = last_values\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - mb_dones[t + 1]\n",
        "                    nextvalues = mb_values[t + 1]\n",
        "                delta = mb_rewards[t] + self.args.gamma * nextvalues * nextnonterminal - mb_values[t]\n",
        "                mb_advs[t] = lastgaelam = delta + self.args.gamma * self.args.tau * nextnonterminal * lastgaelam\n",
        "            mb_returns = mb_advs + mb_values\n",
        "            # after compute the returns, let's process the rollouts\n",
        "            mb_obs = mb_obs.swapaxes(0, 1).reshape(self.batch_ob_shape)\n",
        "            mb_actions = mb_actions.swapaxes(0, 1).flatten()\n",
        "            mb_returns = mb_returns.swapaxes(0, 1).flatten()\n",
        "            mb_advs = mb_advs.swapaxes(0, 1).flatten()\n",
        "            # before update the network, the old network will try to load the weights\n",
        "            self.old_net.load_state_dict(self.net.state_dict())\n",
        "            # start to update the network\n",
        "            pl, vl, ent = self._update_network(mb_obs, mb_actions, mb_returns, mb_advs)\n",
        "            # display the training information\n",
        "            if update % self.args.display_interval == 0:\n",
        "                self.logger.info('[{}] Update: {} / {}, Frames: {}, Rewards: {:.3f}, Min: {:.3f}, Max: {:.3f}, PL: {:.3f},'\\\n",
        "                    'VL: {:.3f}, Ent: {:.3f}'.format(datetime.now(), update, num_updates, (update + 1)*self.args.nsteps*self.args.num_workers, \\\n",
        "                    final_rewards.mean().item(), final_rewards.min().item(), final_rewards.max().item(), pl, vl, ent))\n",
        "                # save the model\n",
        "                torch.save(self.net.state_dict(), self.model_path + '/model.pt')\n",
        "\n",
        "    # update the network\n",
        "    def _update_network(self, obs, actions, returns, advantages):\n",
        "        inds = np.arange(obs.shape[0])\n",
        "        nbatch_train = obs.shape[0] // self.args.batch_size\n",
        "        for _ in range(self.args.epoch):\n",
        "            np.random.shuffle(inds)\n",
        "            for start in range(0, obs.shape[0], nbatch_train):\n",
        "                # get the mini-batchs\n",
        "                end = start + nbatch_train\n",
        "                mbinds = inds[start:end]\n",
        "                mb_obs = obs[mbinds]\n",
        "                mb_actions = actions[mbinds]\n",
        "                mb_returns = returns[mbinds]\n",
        "                mb_advs = advantages[mbinds]\n",
        "                # convert minibatches to tensor\n",
        "                mb_obs = self._get_tensors(mb_obs)\n",
        "                mb_actions = torch.tensor(mb_actions, dtype=torch.float32)\n",
        "                mb_returns = torch.tensor(mb_returns, dtype=torch.float32).unsqueeze(1)\n",
        "                mb_advs = torch.tensor(mb_advs, dtype=torch.float32).unsqueeze(1)\n",
        "                # normalize adv\n",
        "                mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-8)\n",
        "                if self.args.cuda:\n",
        "                    mb_actions = mb_actions.cuda()\n",
        "                    mb_returns = mb_returns.cuda()\n",
        "                    mb_advs = mb_advs.cuda()\n",
        "                # start to get values\n",
        "                mb_values, pis = self.net(mb_obs)\n",
        "                # start to calculate the value loss...\n",
        "                value_loss = (mb_returns - mb_values).pow(2).mean()\n",
        "                # start to calculate the policy loss\n",
        "                with torch.no_grad():\n",
        "                    _, old_pis = self.old_net(mb_obs)\n",
        "                    # get the old log probs\n",
        "                    old_log_prob, _ = evaluate_actions(old_pis, mb_actions)\n",
        "                    old_log_prob = old_log_prob.detach()\n",
        "                # evaluate the current policy\n",
        "                log_prob, ent_loss = evaluate_actions(pis, mb_actions)\n",
        "                prob_ratio = torch.exp(log_prob - old_log_prob)\n",
        "                # surr1\n",
        "                surr1 = prob_ratio * mb_advs\n",
        "                surr2 = torch.clamp(prob_ratio, 1 - self.args.clip, 1 + self.args.clip) * mb_advs\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                # final total loss\n",
        "                total_loss = policy_loss + self.args.vloss_coef * value_loss - ent_loss * self.args.ent_coef\n",
        "                # clear the grad buffer\n",
        "                self.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.max_grad_norm)\n",
        "                # update\n",
        "                self.optimizer.step()\n",
        "        return policy_loss.item(), value_loss.item(), ent_loss.item()\n",
        "\n",
        "\n",
        "    def _get_tensors(self, obs):\n",
        "        obs_tensor = torch.tensor(np.transpose(obs, (0, 3, 1, 2)), dtype=torch.float32)\n",
        "\n",
        "        if self.args.cuda:\n",
        "            obs_tensor = obs_tensor.cuda()\n",
        "        return obs_tensor\n",
        "\n",
        "    def _adjust_learning_rate(self, update, num_updates):\n",
        "        lr_frac = 1 - (update / num_updates)\n",
        "        adjust_lr = self.args.lr * lr_frac\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "             param_group['lr'] = adjust_lr\n"
      ],
      "metadata": {
        "id": "MCRgvIlZIteC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from arguments import get_args\n",
        "from ppo_agent import ppo_agent\n",
        "from models import cnn_net\n",
        "from stable_baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "import gfootball.env as football_env\n",
        "import os\n",
        "\n",
        "\n",
        "def create_single_football_env(args):\n",
        "    \"\"\"Creates gfootball environment.\"\"\"\n",
        "    env = football_env.create_environment(\\\n",
        "            env_name=args.env_name, stacked=True,\n",
        "            )\n",
        "    return env\n",
        "\n",
        "if __name__ == '__main__': \n",
        "\n",
        "    args = get_args()\n",
        "\n",
        "    envs = SubprocVecEnv([(create_single_football_env(args)) for i in range(args.num_workers)], context=None)\n",
        "\n",
        "    network = cnn_net(envs.action_space.n)\n",
        "\n",
        "    ppo_trainer = ppo_agent(envs, args, network)\n",
        "    ppo_trainer.learn()\n",
        "\n",
        "    envs.close()\n"
      ],
      "metadata": {
        "id": "cLTnJSycI5e5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}